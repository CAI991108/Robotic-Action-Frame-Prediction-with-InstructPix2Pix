\section{Research Methodology}
\label{sec:research-methodology}

\subsection{Model Architecture}
\label{sec:model-architecture}

Our framework builds upon the \texttt{InstructPix2Pix}~\cite{brooks2023instructpix2pixlearningfollowimage} architecture
\footnote{InstructPix2Pix GitHub Repository: \url{https://github.com/timothybrooks/instruct-pix2pix/tree/main}}, 
    a pretrained text-conditioned image editing model, 
        which we adapt for robotic action frame prediction through multimodal fine-tuning. 
The model accepts two inputs: 
    (1) a \textbf{current RGB frame} (256$\times$256 resolution) capturing the robot visual observation, and 
    (2) a \textbf{textual action instruction} (e.g., “beat the block with the hammer”). 
These inputs are processed as follows:
\begin{enumerate}
    \item \textbf{Input Adaptation:}
        \begin{itemize}
            \item \textbf{Visual Encoder}: 
            The input frame is encoded into a latent representation via a \textit{UNet-based image encoder}, 
                initialized from the pretrained \texttt{InstructPix2Pix}.
            \item \textbf{Text Encoder}:
            Action instructions are tokenized and embedded using the \textit{CLIP Text Encoder}, 
                which aligns language semantics with visual features.
            \item \textbf{Multimodal Fusion}:
            The encoded image and text features are concatenated and fed into the diffusion model cross-attention layers, 
                enabling joint reasoning over visual scenes and language instructions.
        \end{itemize}
    \item \textbf{Output Optimization:}
    To generate high-resolution future frames (256$\times$256), we retain the original \textit{Stable Diffusion VAE decoder} 
        but adjust the UNet upsampling layers to ensure compatibility with the target resolution. 
    This modification is guided by the \codeblock{image\_size} and \codeblock{crop\_res} 
        parameters in the training configuration (see \codeblock{train.yaml}).
\end{enumerate}

\subsection{Training Procedure}
\label{sec:training-procedure}

To address the challenges of limited training data (300 samples) and ensure stable convergence, 
    we employ the following strategies:
\begin{enumerate}
    \item \textbf{Loss Functions:}
    \begin{itemize}
        \item \textbf{LPIPS (Learn Perceptual Image Patch Similarity)}: 
        Measures perceptual similarity between generated and ground-truth frames.
        \item \textbf{L1 Pixel Loss}: 
        \begin{equation}
            \mathcal{L} = \lambda_{LPIPS} \cdot \mathcal{L}_{LPIPS} + \lambda_{L1} \cdot \mathcal{L}_{L1}
        \end{equation}
        Enforces pixel-level reconstruction accuracy with the combined loss, 
            balancing the structural and low-level fidelity of the predicted frames. 
        Here, $\lambda_{LPIPS}$ and $\lambda_{L1}$ are hyperparameters that control the trade-off between perceptual quality and pixel-wise accuracy.
    \end{itemize}
    \item \textbf{Regularization:}
    \begin{itemize}
        \item \textbf{Dropout (p=0.2)}:
        Applied to the UNet intermediate layers to mitigate overfitting.
        \item \textbf{Data Augmentation}:
        Random cropping and brightness jitter (±20\% of original luminance) simulate environmental variations.
    \end{itemize}
    \item \textbf{Efficiency Optimization:}
    \begin{itemize}
        \item \textbf{FP16 Mixed Precision Training}:
        Reduces GPU memory usage by 40\% and accelerates training throughput, enabled via PyTorch \codeblock{autocast} API.
        \item \textbf{Batch Configuration}:
        A batch size of 2 and gradient accumulation over 8 steps (equivalent to an effective batch size of 16) 
            ensure stable training under memory constraints (see \codeblock{train.yaml}).
    \end{itemize}
\end{enumerate}

\subsection{Data Preprocessing}
\label{sec:data-preprocessing}

The dataset is synthesized using \texttt{RoboTwin}~\cite{mu2025robotwindualarmrobotbenchmark}
\footnote{RoboTwin Dual-Arm Robot Benchmark with Generative Digital Twins: \url{https://github.com/TianxingChen/RoboTwin/tree/main}}, 
    a virtual robotic simulation environment, and processed as follows:
\begin{enumerate}
    \item \textbf{Dataset Generation:}
    \begin{itemize}
        \item \textbf{Task-Specific Samples}:
        We generate 100 episodes for each of the following three tasks 
            (\textit{block\_hammer\_beat, block\_handover, blocks\_stack\_easy}), yielding 300 total samples. 
        Each episode includes RGB frames intervals, paired with corresponding textual instructions.
        \item \textbf{Frame Extraction}:
        Randomly selects the input frame (\textit{initial-50 observation}) and the target frame (\textit{last-50 future frames}), 
            then maps them to instructions using \codeblock{TASK\_TO\_INSTRUCTION}.
    \end{itemize}
    \item \textbf{Preprocessing Pipeline:}
    \begin{itemize}
        \item \textbf{JSONL Conversion}:
        The images and instructions are paired \codeblock{instructpix2pix\_dataset.jsonl}.
        \item \textbf{Structured Dataset Assembly}:
        The built-in python scrpit \codeblock{data\_prepare.py} converts JSONL entries into the InstructPix2Pix-compatible format,
            where input and target frames are renamed as \textit{{seed}\_0.jpg} and \textit{{seed}\_1.jpg}, 
            together with the instructions are stored in \textit{prompt.json} files, respectively.
        \item \textbf{Data Splitting}:
        The final dataset is partitioned into training (90\%) and validation (10\%), 
            specified in \codeblock{train.yaml} (data.params.train.params.path).
    \end{itemize}
\end{enumerate}

The above pipeline ensures seamless integration with the \texttt{InstructPix2Pix} training framework, 
    enabling direct compatibility with its data loaders and diffusion processes.