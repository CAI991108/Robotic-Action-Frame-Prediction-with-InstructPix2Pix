\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Full Parameters Settings and Training Logs}
\label{sec:appendix}

\begin{table}[htbp]
    \centering
    %\caption{Experimental Configuration}
    \label{tab:exp_config}
    \begin{tabular}{p{0.2\textwidth} p{0.3\textwidth} p{0.5\textwidth}}
      \toprule
      \textbf{Category} & \textbf{Parameter} & \textbf{Value} \\
      \midrule
      \multirow{4}{*}{\textbf{Hardware}} & GPUs & 2 $\times$ NVIDIA RTX 2080 Ti, \newline 22GB VRAM each, \newline CUDA 12.2, \newline Turing Architecture \\
       & CPU & 18 cores (36 threads) \\
       & RAM & 134.73 GB \\
       & Disk & 2.01 TB total, 131.32 GB used \\
      \midrule
      \multirow{3}{*}{\textbf{Software}} & OS & Linux 6.8.0-52-generic \\
       & Python & 3.8.20 (CPython) \\
       & Framework & PyTorch Lightning 1.9.0, DDP accelerator \\
      \midrule
      \multirow{5}{*}{\textbf{Training Setup}} & Batch Size & 2 per GPU, 8 gradient accumulation steps \newline $\rightarrow$ effective batch size = 16 \\
       & Optimizer & AdamW, learning rate = 1e-4 \\
       & Training Epochs & 100 \\
       & Training Time & $\sim$3.14 hours (11,292 seconds) \\
       & Checkpointing & \texttt{logs/train\_default/checkpoints/last.ckpt} \\
      \midrule
      \multirow{3}{*}{\textbf{Model Architecture}} & Base Model & instructPix2Pix \newline (Stable Diffusion v1.5 fine-tuned) \\
       & UNet Configuration & 320 model channels, \newline 8 attention heads, \newline 768 context dim \\
       & VAE Decoder & AutoencoderKL (256$\times$256 resolution) \\
      \midrule
      \multirow{7}{*}{\textbf{Dataset}} & Source & RoboTwin-generated synthetic data \\
       & Tasks & \texttt{block\_hammer\_beat}, \newline \texttt{block\_handover}, \newline \texttt{blocks\_stack\_easy} \\
       & Samples & 300 (100 per task) \\
       & Resolution & 256$\times$256 \\
       & Augmentation & Random cropping, \newline brightness jitter ($\pm$20\%) \\
       & Train Loss (Simple) & 0.055 \\
       & Validation Loss (Simple) & 0.0387 \\
       & Validation PSNR & 37.8-40.1 dB (task-dependent) \\
       & Validation SSIM & 0.95-0.98 (task-dependent) \\
       & Peak GPU Memory Usage & $\sim$17.7 GB per GPU \\
       & Avg. Epoch Time & 26-32 seconds \\
      \midrule
      \multirow{3}{*}{\textbf{Reproducibility}} & Code Path & \codeblock{main.py} (PyTorch Lightning) \\
       & Environment & Conda env \texttt{ip2p}, \newline dependencies in \codeblock{environment.yaml} \\
       & W\&B Tracking & Logged metrics: \newline loss, SSIM, PSNR, \newline GPU/CPU utilization \\
      \bottomrule
    \end{tabular}
  \end{table}